\section{Adaptive Signalverarbeitung}

\subsection{Adaptive Filter}
\textbf{Ziel:} Verarbeitungseigenschaften des adaptiven Systems selbstständig durch rekursive Modifikation seiner Parameter $c_n$ ändern.

\subsubsection{LS (Least Square) Algorithmus - Wiener Lösung}
Für Berechnung der AKF $Rss(i)$ und der KKF $Rsr(i)$ werden statische Eigenschaften des Zufallsprozesses verwendet (Erwartungswerte). D.h. das Eingangssignal $s(i)$ und das Referenzsignal $r(i)$ werden in gleichlange Blöcke aufgeteilt.\\ 
$\begin{bmatrix}
    Rss(0) & Rss(1) & Rss(2) & \cdots & Rss(N) \\
    Rss(1) & Rss(0) & Rss(1) & \cdots & Rss(N-1)\\
    Rss(2) & Rss(1) & Rss(0) & \cdots & Rss(N-2) \\
    \vdots & \vdots & \vdots &  \vdots & \vdots \\
    Rss(N) & Rss(N-1) & Rss(N-2) & \cdots & Rss(0)
\end{bmatrix}
\cdot
\begin{bmatrix}
    c_0 \\
    c_1 \\
    c_2 \\
    \vdots \\
    c_N
\end{bmatrix}
=
\begin{bmatrix}
    Rsr(0) \\
    Rsr(1) \\
    Rsr(2) \\
    \vdots \\
    Rsr(N)
\end{bmatrix}$
$\qquad Rss \cdot c = Rsr \Longrightarrow c=Rss^{-1} \cdot Rsr$ 


\subsubsection{RLS (Recursive Least Square ) Algorithmus}
\textbf{Idee:} $Rss \cdot c = Rsr$ rekursiv lösen (schneller)\\
Die AKF und KKF werden direkt aus den anliegenden Signalen berechnet (keine Erwartungswerte).

$\text{Filterordnung: }p \qquad \text{Signalwerte: } s(i) = [x(n)~x(n-1) \cdots x(n-p)]^T \qquad c(i)=P(i) \cdot Rsr(i) \qquad P(i) = Rss^{-1}(i)$\\
Exponentielles Vergessen: Vergessensfaktor $\mu \quad (0<\mu<1) \qquad \text{Wahl: }(\mu \in [0.9,\cdots,0.9999]\text{ ;} \quad \text{ohne Vergessen } \mu=1)$\\
Initialisierung: $i=1 \qquad P(0) = I\cdot \lambda \quad (\lambda >> 1) \qquad c(0) = 0 \text{ (oder guter Schätzwert)}$
\begin{enumerate}
 \item Gain: $k(i) = \frac{P(i-1) \cdot s(i)}{\mu + s^T(i) \cdot P(i-1) \cdot s(i)} \qquad \text{Zeilenvektor}$
 \item Schätzfehler: $\eta(i) = r(i) - s^T(i) \cdot c(i-1) \qquad \text{(Zahl)}$
 \item Koeffizienten: $c(i) = c(i-1) + k(i) \cdot \eta(i) \qquad \text{(Zeilenvektor)}$
 \item Fehlerkorrelationsmatrix: $P(i) = \frac{1}{\mu} \left( P(i-1) - k(i) \cdot s^T(i) \cdot P(i-1) \right) \qquad \text{(Matrize)}$
\end{enumerate}


\subsubsection{LMS (Least Mean Square) Algorithmus - Gradienten Algo.}
Koeffizienten werden schrittweise berechnet. Sie werden in richtung des negativen Gradienten des quadratischen Fehlers korrigiert. Standard LMS ohne Mittelwertbildung!
(Vorteil: weniger rechenintensiv als RLS; Nachteil: konvergiert 10 mal langsamer als RLS)
\begin{enumerate}
 \item $f(i) = c_0(i-1)\cdot s(i) + c_1(i-1) \cdot s(i -1) + \cdots + c_N(i-1) \cdot s(i-N)$
 \item $e(i) = r(i) - f(i)$
 \item $c_n(m+1)=c_n(m) + \alpha\cdot e(i) \cdot s(i-n) \qquad \text{[$\alpha$ gross $\rightarrow$ wird instabil; $\alpha$ klein $\rightarrow$ konvergiert langsam]}$
\end{enumerate}

\textbf{Normalized Least Mean Square (NLMS):} $\alpha(i) = \frac{\alpha_0}{\delta + L \cdot \sigma^2} \qquad L=N+1$\\
$\delta$: kleiner Wert (verhindert Div durch 0); Korrekturkonstante $\alpha$ umgekehrt proportional zur Signalleistung $\sigma^2$
